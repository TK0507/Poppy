[REPORT 2024/08/01 17:48]
[AUTHOR] 木村　拓耶
[NOTE] このテキストは当日における実際の作業内容についての日々の記録です

section-1 今回の作業結果 {

    今回の作業で特筆すべき改善が実施されました。
    今回の作業によって大幅なリファクタリングが実施された影響で、
    既存のエンティティの実装や役割、およびインターフェースが破壊的に変更されました。

    > csvの形式を変更し、学習速度が格段に向上しました。

    前回まではテキストとスコアのペアをcsvに保持していましたが、
    今回の更新でテキストに対するBERT出力とスコアをcsv内に直接保持するようになりました。
    その影響で、前回まで学習時にBERT出力を取得して学習を実施していた部分が改善され、
    BERT出力を実行せずとも学習可能となったため、オーバーヘッドが大幅に削減されました。

    > 不要なテンソル階層を削除し、データのアンラップが容易になりました。

    前回までの冗長なプログラム部分を削除しました。

    > いくつかのクラス及びメソッドの実装及びインターフェースが変更されました

    詳細はnaming-20240801.txtを参照してください。

    > モデルの中間層の活性化関数がSoftsignからMishに変更されました。

    前回までは、負のパラメータを保持することを考慮しており、
    TanhまたはSoftsignを中間層の活性化関数として用いて実験していました。
    しかし、実際のところ負の出力を保持する際には重みが正常に負に更新されることで実質的に可能であるため、
    これらの活性化関数を意図的に用いることにはあまり効果がないことが判明しました。
    同様に、Mish関数に置き換えた実験でTanhおよびSoftsignよりも少ないステップで、
    しかも精度の高い学習を実施できたため、これらの活性化関数の中間層での利用は再検討されます。
    以降、特に変更理由のない限り中間層での活性化関数はReLU様の関数が利用されます。
}